{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit"
  },
  "interpreter": {
   "hash": "51ddf0e706720420094069fdfda5ce527a09d72184f71302b5d55f33a7e47bec"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# KNN With Single Label\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Assigning features and label variables\r\n",
    "# First Feature\r\n",
    "weather=['Sunny','Sunny','Overcast','Rainy','Rainy','Rainy','Overcast','Sunny','Sunny',\r\n",
    "'Rainy','Sunny','Overcast','Overcast','Rainy']\r\n",
    "# Second Feature\r\n",
    "temp=['Hot','Hot','Hot','Mild','Cool','Cool','Cool','Mild','Cool','Mild','Mild','Mild','Hot','Mild']\r\n",
    "\r\n",
    "# Label or target varible\r\n",
    "play=['No','No','Yes','Yes','Yes','No','Yes','No','Yes','Yes','Yes','Yes','Yes','No']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Import Label Encoder \r\n",
    "from sklearn import preprocessing\r\n",
    "\r\n",
    "# Creating Label Encoder\r\n",
    "le = preprocessing.LabelEncoder()\r\n",
    "\r\n",
    "# Convert the all the strings into numbers\r\n",
    "weather_encoded = le.fit_transform(weather)\r\n",
    "temp_encoded = le.fit_transform(temp)\r\n",
    "label = le.fit_transform(play)\r\n",
    "\r\n",
    "# Print the encoded weather list\r\n",
    "print(weather_encoded)\r\n",
    "print(temp_encoded)\r\n",
    "print(label)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2 2 0 1 1 1 0 2 2 1 2 0 0 1]\n",
      "[1 1 1 2 0 0 0 2 0 2 2 2 1 2]\n",
      "[0 0 1 1 1 0 1 0 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Combine Weather and Temp into a singular list of tuples - features\r\n",
    "features = list(zip(weather_encoded, temp_encoded))\r\n",
    "\r\n",
    "print(features)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(2, 1), (2, 1), (0, 1), (1, 2), (1, 0), (1, 0), (0, 0), (2, 2), (2, 0), (1, 2), (2, 2), (0, 2), (0, 1), (1, 2)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Import the KNN classifier model \n",
    "### 2. Create the KNN classifier object by passing argument number of neighbors in KNeighborsClassifier() function.\n",
    "### 3. Then, fit your model on the train set using fit() \n",
    "### 4. Perform prediction on the test set using predict(). \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "model = KNeighborsClassifier(n_neighbors=3)\r\n",
    "\r\n",
    "# Train the model using training datasets\r\n",
    "model.fit(features, label)\r\n",
    "\r\n",
    "# Predict the output scenario where 0:Overcast and 2:Mild\r\n",
    "predicted = model.predict([[0,2]])\r\n",
    "\r\n",
    "print(predicted)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# KNN with Multiple Labels\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from sklearn import datasets\r\n",
    "\r\n",
    "# Load Wine Datasets\r\n",
    "wine = datasets.load_wine()\r\n",
    "\r\n",
    "#wine"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# print the names of the features\r\n",
    "print(wine.feature_names)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# print the name of the labels\r\n",
    "print(wine.target_names)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['class_0' 'class_1' 'class_2']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Check the wine dataset\r\n",
    "print(wine.data[0:5])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n",
      "  2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.320e+01 1.780e+00 2.140e+00 1.120e+01 1.000e+02 2.650e+00 2.760e+00\n",
      "  2.600e-01 1.280e+00 4.380e+00 1.050e+00 3.400e+00 1.050e+03]\n",
      " [1.316e+01 2.360e+00 2.670e+00 1.860e+01 1.010e+02 2.800e+00 3.240e+00\n",
      "  3.000e-01 2.810e+00 5.680e+00 1.030e+00 3.170e+00 1.185e+03]\n",
      " [1.437e+01 1.950e+00 2.500e+00 1.680e+01 1.130e+02 3.850e+00 3.490e+00\n",
      "  2.400e-01 2.180e+00 7.800e+00 8.600e-01 3.450e+00 1.480e+03]\n",
      " [1.324e+01 2.590e+00 2.870e+00 2.100e+01 1.180e+02 2.800e+00 2.690e+00\n",
      "  3.900e-01 1.820e+00 4.320e+00 1.040e+00 2.930e+00 7.350e+02]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Check the wine labels\r\n",
    "print(wine.target)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Check the feature shape of the dataset\r\n",
    "print(wine.data.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(178, 13)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Check the label shape\r\n",
    "print(wine.target.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(178,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Splitting Data\n",
    "To understand model performance, dividing the dataset into a training set and a test set is a good strategy.\n",
    "\n",
    "Let's split dataset by using function train_test_split(). You need to pass 3 parameters features, target, and test_set size. Additionally, you can use random_state to select records randomly."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# import train_test_split function\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "# Split the dataset into a training and testing set\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.3) # 70% as training set and 30% as testing set\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generating Model for K=5\n",
    "Let's build KNN classifier model for k=5."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Import knearest neighbors Classifier model\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "# Create KNN Classifier\r\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\r\n",
    "\r\n",
    "# Train the model using the training sets\r\n",
    "knn.fit(X_train, y_train)\r\n",
    "\r\n",
    "# Predict the response for the test dataset\r\n",
    "y_prediction = knn.predict(X_test)\r\n",
    "\r\n",
    "print(y_prediction)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1 1 0 0 1 1 2 1 1 2 1 2 1 0 1 0 1 0 2 1 0 2 0 0 0 2 0 2 1 1 2 0 2 1 0 1 2\n",
      " 1 2 0 1 2 2 2 1 2 2 1 1 2 0 0 0 1]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Evaluation for k=5\n",
    "Let's estimate, how accurately the classifier or model can predict the type of cultivars.\n",
    "\n",
    "Accuracy can be computed by comparing actual test set values and predicted values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# import scikit-learn metrics module for accuracy calculation\r\n",
    "from sklearn import metrics\r\n",
    "\r\n",
    "# Model Accuracy, how often is the classifier correct?\r\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_prediction))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy:  0.6666666666666666\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generating Model for K=7\n",
    "Let's build KNN classifier model for k=7"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Import knearest neighbors Classifier model\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "# Create KNN Classifier\r\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\r\n",
    "\r\n",
    "# Train the model using the training sets\r\n",
    "knn.fit(X_train, y_train)\r\n",
    "\r\n",
    "# Predict the response for the test dataset\r\n",
    "y_prediction = knn.predict(X_test)\r\n",
    "\r\n",
    "print(y_prediction)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1 1 0 0 1 2 2 1 1 2 1 1 1 0 1 0 1 0 2 1 0 2 0 0 0 1 0 2 2 1 2 0 2 1 0 1 2\n",
      " 1 2 0 1 2 2 2 1 2 2 1 1 2 0 0 2 1]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Evaluation for k=7"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# import scikit-learn metrics module for accuracy calculation\r\n",
    "from sklearn import metrics\r\n",
    "\r\n",
    "# Model Accuracy, how often is the classifier correct?\r\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_prediction))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy:  0.6481481481481481\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Well, you got a classification rate of 75.92%, considered as good accuracy.\n",
    "\n",
    "Here, you have increased the number of neighbors in the model and accuracy got increased. But, this is not necessary for each case that an increase in many neighbors increases the accuracy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pros\n",
    "1. The training phase of K-nearest neighbor classification is much faster compared to other classification algorithms. \n",
    "2. There is no need to train a model for generalization, that is why KNN is known as the simple and instance-based learning algorithm. \n",
    "3. KNN can be useful in case of nonlinear data. It can be used with the regression problem. \n",
    "4. Output value for the object is computed by the average of k closest neighbors value.\n",
    "\n",
    "## Cons\n",
    "1. The testing phase of K-nearest neighbor classification is slower and costlier in terms of time and memory. \n",
    "2. It requires large memory for storing the entire training dataset for prediction. \n",
    "3. KNN requires scaling of data because KNN uses the Euclidean distance between two data points to find nearest neighbors. Euclidean distance is sensitive to magnitudes. The features with high magnitudes will weight more than features with low magnitudes. \n",
    "4. KNN also not suitable for large dimensional data."
   ],
   "metadata": {}
  }
 ]
}